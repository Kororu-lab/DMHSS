{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kororu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import ElectraTokenizer, ElectraForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = 'gooohjy/suicidal-electra'\n",
    "model = ElectraForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = ElectraTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Specify device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)\n",
    "\n",
    "def get_label_probabilities(text):\n",
    "    # Tokenize the text\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "\n",
    "    # Move inputs to device\n",
    "    inputs = {name: tensor.to(device) for name, tensor in inputs.items()}\n",
    "    \n",
    "    # Get the model outputs\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Get the probabilities with softmax\n",
    "    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    \n",
    "    # Move probs back to CPU if necessary for further processing\n",
    "    probs = probs.cpu().numpy()[0]\n",
    "    \n",
    "    return probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suicide: 0.99925, non_suicide: 0.00075\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "text = \"It is such a wonderful day to kill myself and bring me a peace.\"\n",
    "probs = get_label_probabilities(text)\n",
    "print(f'suicide: {probs[1]:.5f}, non_suicide: {probs[0]:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twitter Sentiment Score: 0.9901023507118225\n",
      "Kaggle Model 2 Score: 0.9518818259239197\n"
     ]
    }
   ],
   "source": [
    "import twitter_sentiment\n",
    "import kaggle_model_2\n",
    "\n",
    "# Initialize the models\n",
    "twitter_sentiment.initialize_model()\n",
    "kaggle_model_2.initialize_model()\n",
    "\n",
    "# Test text\n",
    "text = \"\"\"I want to be erased from everyone's memory\n",
    "I want to fall asleep and not wake up\n",
    "I want to be murderd so I'm not to blame \n",
    "I just want to escape\n",
    "I cut every night because my step dad\n",
    "Fuck him\"\"\"\n",
    "\n",
    "# Get label probabilities\n",
    "twitter_score = twitter_sentiment.get_label_probabilities(text)\n",
    "kaggle_score = kaggle_model_2.get_label_probabilities(text)\n",
    "\n",
    "# Print the scores\n",
    "print(f\"Twitter Sentiment Score: {twitter_score}\")\n",
    "print(f\"Kaggle Model 2 Score: {kaggle_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "import torch\n",
    "\n",
    "model_name = 'rafalposwiata/deproberta-large-depression'\n",
    "model = None\n",
    "tokenizer = None\n",
    "device = None\n",
    "\n",
    "def initialize_model():\n",
    "    global model, tokenizer, device\n",
    "    model = RobertaForSequenceClassification.from_pretrained(model_name)\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "def get_label_probabilities(text):\n",
    "    if model is None or tokenizer is None:\n",
    "        initialize_model()\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "    inputs = {name: tensor.to(device) for name, tensor in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    probs = probs.cpu().numpy()[0]\n",
    "    \n",
    "    # Returning the probability of label 1 (suicide)\n",
    "    return [probs[0], probs[1], probs[2]]  # [label_0_score, label_1_score]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twitter Sentiment Score: 0.08310867100954056\n"
     ]
    }
   ],
   "source": [
    "# Initialize the models\n",
    "initialize_model()\n",
    "\n",
    "# Test text\n",
    "text = \"\"\"Well, it is time to the end. I QUIT\"\"\"\n",
    "\n",
    "# Get label probabilities\n",
    "twitter_score = get_label_probabilities(text)\n",
    "\n",
    "# Print the scores\n",
    "print(f\"Twitter Sentiment Score: {twitter_score[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# severe, moderate, not severe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
